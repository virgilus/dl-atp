{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e256aa17-a52a-40bb-ae65-049e51b53cc7",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd3e22-0221-4d4b-9418-949cb77ef51b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/deep_learning_brain.jpg\" width=\"95%\" source='https://www.futura-sciences.com/tech/definitions/intelligence-artificielle-deep-learning-17262/' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7646f70-cc78-4e37-b98b-4834b158b589",
   "metadata": {},
   "source": [
    "Nowadays, the deep learning is everywhere:\n",
    "\n",
    "- Autonomous cars\n",
    "- Unlocking phones\n",
    "- Person tracking\n",
    "- Detection of multiple sources in sounds\n",
    "- Captioning (picture to text)\n",
    "- Generating text (Large Language Model)\n",
    "- Generate images\n",
    "- Generate videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a273380-afd9-431f-a133-c1aa6a503332",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a58493-fad1-4887-a396-182b221047e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d83962-0c3d-41b8-a250-420a483323bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input X = one single observation, 4 features (x1, x2, x3, x4)\n",
    "# (e.g. eyes color, ears_lenghts, ...)\n",
    "X = [1., -3.1, -7.2, 2.1]\n",
    "\n",
    "# Target y (classification task 0/1, e.g. cat/dog)\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789ba58-0746-4bbb-9d7e-f9b666f2a068",
   "metadata": {},
   "source": [
    "Imagine you have a **linear regression** with some weights :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66adf2f-8060-4ed9-8888-27793fc7649c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linreg_1(X):\n",
    "    return -3 + 2.1*X[0] - 1.2*X[1] + 0.3*X[2] + 1.3*X[3]\n",
    "\n",
    "out_1 = linreg_1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b6053-93f0-48e4-86d9-a49582756943",
   "metadata": {},
   "source": [
    "And you transform its output with an **activation function**. So we're making a linear function non linear :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4570aa4-8e51-4e3b-8be6-186da190e3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def activation(value):\n",
    "    if value > 0:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "out_1 = activation(out_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a698e4-6139-4f45-8401-f33f2919c774",
   "metadata": {},
   "source": [
    "## Neuron (perceptron)\n",
    "\n",
    "### Weights\n",
    "\n",
    "This **neuron**, also called **perceptron**, is made of the **sum of weighted features** combined with an **activation function**.\n",
    "\n",
    "We took our 4 features ($x_1$, $x_2$, $x_3$, and $x_4$), we applied some weights, we summed everything and if the result is positive it returns the value, otherwise, it returns 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be4a4a-fd0b-4978-a684-b470ed5f4566",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/neuron_1.png\" width=\"55%\" source='https://www.researchgate.net/figure/12-Schema-de-structure-dun-seul-neurone-De-facon-simplifiee-le-reseau-de-neurones_fig3_324474500' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e48e8d-fab2-4f28-bb47-26779d7b0de2",
   "metadata": {},
   "source": [
    "Given an input $X = (x_1, x_2, \\ldots, x_n)$, a neuron is the concatenation of:\n",
    "\n",
    "1. A **linear combination** of the input with the weights $w_k$ plus a bias $b$, that outputs $\\sum_{k=1}^n w_k x_k + b$.\n",
    "\n",
    "2. A **non-linear modification** $f$ of that sum.\n",
    "\n",
    "Therefore, the output of a neuron is $\\text{output} = f\\left(\\sum_{k=1}^n w_k x_k + b\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c3dec-d47b-413b-b794-555dcc27be9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "There are many different activation functions.\n",
    "<div>\n",
    "<img src=\"files/activation_functions.png\" width=\"55%\" source='https://www.researchgate.net/figure/12-Schema-de-structure-dun-seul-neurone-De-facon-simplifiee-le-reseau-de-neurones_fig3_324474500' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fefc1-b864-4e6a-8062-c8bd63896b49",
   "metadata": {},
   "source": [
    "Note : $tanh(x)$ is also called hyperbolic tangent.\n",
    "\n",
    "‚ùì **Which of these activations did we just code?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e29c36-2b59-4a04-8d2b-c660d44160a2",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "Now imagine now that you produce another output by:\n",
    "\n",
    "- Applying **another linear regression** to the same input X.\n",
    "- Followed by the same activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3a4ae-f042-4565-bb28-5c2861bc9de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Second neuron\n",
    "def linreg_2(X):\n",
    "    return -5 + (-0.1*X[0]) + 1.2*X[1] + 4.9*X[2] - 3.1*X[3] # Same X but different weights\n",
    "\n",
    "out_2 = activation(linreg_2(X)) # Same activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41322f-d9b8-4a10-ab78-b63a8fd5dd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Third neuron\n",
    "def linreg_3(X):\n",
    "    return -8 + 0.4*X[0] + 2.6*X[1] + (-2.5*X[2]) + 3.8*X[3] # Same X but different weights\n",
    "\n",
    "out_3 = activation(linreg_3(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62d487-435e-47a7-bb1b-a84750ba8777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fourth neuron\n",
    "def linreg_4(X):\n",
    "    return 10 + 0.8*X[0] + 1.7*X[1] + 3.5*X[2] + 1.67*X[3]\n",
    "\n",
    "out_4 = activation(linreg_3(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c35eb1-b013-42e4-a5eb-539ee13266cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fifth neuron\n",
    "def linreg_5(X):\n",
    "    return -7.3 + 0.42*X[0] + 3.89*X[1] + (-0.675*X[2]) + (-13.8*X[3])\n",
    "\n",
    "out_5 = activation(linreg_3(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197eb377-5a55-4198-b380-09f1d37e3c40",
   "metadata": {},
   "source": [
    "Now we have created 5 neurons : ```out_1```, ```out_2```, ```out_3```, ```out_4``` and ```out_5```. Each one takes the same inputs (X), but the linear regression functions have different coefficients. They also have the same activation function.\n",
    "\n",
    "(**Note**: Each neuron can have a different activation function but in practice, each layer only uses one type.)\n",
    "\n",
    "**We just wrote a layer of neurons !**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b001e-1b06-4b90-9d08-fc7ad65345d2",
   "metadata": {},
   "source": [
    "There are 5 neurons on this graph, but we just created 3 more.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/layer.png\" width=\"55%\" source='https://www.researchgate.net/figure/12-Schema-de-structure-dun-seul-neurone-De-facon-simplifiee-le-reseau-de-neurones_fig3_324474500' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0e160-91fc-4a64-8006-78f91c0519af",
   "metadata": {},
   "source": [
    "### What if we use the 3 outputs of this layer as input of another layer, again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310e8cd-1553-4a35-b5c9-ed33222cffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linreg_second_layer(X):\n",
    "    return 5.1 + 1.1*X[0] - 4.1*X[1] - 0.7*X[2] + 1.7*X[3] + (-8.91*X[4]) # Now we have 5 inputs!\n",
    "\n",
    "def activation_second_layer(value):\n",
    "    # sigmoid activation (for classification task for example)!\n",
    "    return 1. / (1 + np.exp(-value))\n",
    "\n",
    "def neural_net_predictor(X):\n",
    "    \n",
    "    # First layer\n",
    "    out_1 = activation(linreg_1(X))\n",
    "    out_2 = activation(linreg_2(X))\n",
    "    out_3 = activation(linreg_3(X))\n",
    "    out_4 = activation(linreg_4(X))\n",
    "    out_5 = activation(linreg_5(X))\n",
    "    \n",
    "    outs = [out_1, out_2, out_3, out_4, out_5] # All outputs from layer 1\n",
    "    \n",
    "    # Second layer and prediction\n",
    "    y_pred = activation_second_layer(linreg_second_layer(outs))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09457903-f1e5-48db-9853-9f16943277cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neural_net_predictor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d7119-f8ae-41fc-aacc-2d1fd12fff55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## So, what is a Neural Network?\n",
    "\n",
    "Nothing more than a fancy function $f_{\\theta}$ that computes $\\hat{y} = f_{\\theta}(x)$, where $\\theta$ are the weights of all the linear regressions that take place within the neurons.\n",
    "\n",
    "Usually:\n",
    "\n",
    "- $\\theta$ means are all the weights, including $b$.\n",
    "- $w$ is the weights without the $b$.\n",
    "- $b$ are the intercepts, also called the biais. (And called $\\beta_0$ in a regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1301c-ceff-478f-aa82-136d34a2d5c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/neuralnet.png\" width=\"55%\" source='https://www.researchgate.net/figure/12-Schema-de-structure-dun-seul-neurone-De-facon-simplifiee-le-reseau-de-neurones_fig3_324474500' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f38c8c-55bd-474f-b1f4-32f95392a022",
   "metadata": {},
   "source": [
    "In the above graph, how many weights are in the hidden layer and output layer?\n",
    "\n",
    "- There are 5$x$, and 4 neurons. So 20 $w$. But you've got also an intercept ($b$) in each one of the neuron so 24. So $\\theta$ is 24 for the first layer.\n",
    "- In the output layer, there are 4 different results + the biais ($b$), so 5 weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae013ca-ed18-4273-b8bb-f3b69475092b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deep learning and neural networks\n",
    "\n",
    "\"**Deep learning**\" means we're using a neural networks that have many layers.\n",
    "\n",
    "Of course the output can be **one value** or **several values**. For example you can output each pixel of a new image. Or if we have 10 classes, we can predict 10 probabilities (and the sum will be 1).\n",
    "\n",
    "When all neurons are connected to each other we call it \"dense\".\n",
    "\n",
    "<div>\n",
    "<img src=\"files/neuralnet_2.png\" width=\"55%\" source='https://www.researchgate.net/figure/12-Schema-de-structure-dun-seul-neurone-De-facon-simplifiee-le-reseau-de-neurones_fig3_324474500' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bd7b8-98a5-4fac-9d62-ec4142b2222e",
   "metadata": {},
   "source": [
    "## Why do we use activation functions?\n",
    "\n",
    "To introduce non-linearities! Without them, our Neural Network would be a simple linear model.\n",
    "\n",
    "$A(a_1x_1 + a_2x_2) + B(b_1x_1 + b_2x_2) = (Aa_1 + Bb_1)x_1 + (Aa_2 + Bb_2)x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27716f2-0a8f-4856-9521-1457dcdc3a3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tensorflow Playground\n",
    "\n",
    "[Tensor Flow Playground](https://playground.tensorflow.org/)\n",
    "\n",
    "With the default parameters.\n",
    "\n",
    "- The orange color means it's negative and the blue means it's positive.\n",
    "- The second 4 features are created with the 2 initial features, with a Tanh activation and a biais.\n",
    "\n",
    "If we try with a linear activation, the algorithm can't find a way to separate my data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a49be-5bba-41ac-96ff-e0b552228d56",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "**Keras** started out as an independent library, but a few years ago got backed-up by Google. The library is included (as a separate & totally different package) in **TensorFlow**, which is also a deep learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858c887-aeee-4fdf-afdb-b32005dcda7e",
   "metadata": {},
   "source": [
    "<img src=\"files/keras_and_tf.png\" width=\"55%\" source='https://www.researchgate.net/figure/12-Schema-de-structure-dun-seul-neurone-De-facon-simplifiee-le-reseau-de-neurones_fig3_324474500' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db8c10-c27e-4dcc-b571-aeb48d6e50f4",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install keras use:\n",
    "    \n",
    "```pip install tensorflow```\n",
    "\n",
    "And to import:\n",
    "    \n",
    "```from tensorflow.keras import *```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82506e2-d0f0-4c25-bf31-6b07b15893ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "# Basically, it will look like a sequence of layers \n",
    "model = Sequential()\n",
    "\n",
    "# First layer: 10 neurons and ReLU as the activation function\n",
    "model.add(layers.Dense(10, activation='relu')) \n",
    "\n",
    "# The standard layers are called Fully Connected (Dense in Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298966be-5a9b-450f-9282-451808ec67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can go for two fully connected layers\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fdaba-2e17-4dd0-abcb-b9c4748c9818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also go for many, many, many more ...\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='tanh'))\n",
    "model.add(layers.Dense(10, activation='linear'))\n",
    "model.add(layers.Dense(100, activation='sigmoid'))\n",
    "model.add(layers.Dense(40, activation='softmax'))\n",
    "model.add(layers.Dense(10, activation='tanh'))\n",
    "model.add(layers.Dense(3, activation='relu'))\n",
    "model.add(layers.Dense(9, activation='tanh'))\n",
    "model.add(layers.Dense(8900, activation='relu'))\n",
    "model.add(layers.Dense(1000, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcb186-6e29-454e-b5f2-ea5e5f9d1f9e",
   "metadata": {},
   "source": [
    "## Decision rules\n",
    "\n",
    "How should I know how many layers, with how many nodes, and what activation function should I use?\n",
    "\n",
    "Well, it takes a lot of time and effort but you can start with these simple rules:\n",
    "\n",
    "### Rule N¬∞1 : The first layer\n",
    "\n",
    "Your first layer should be the size of your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea7b5c-1c0d-4d5a-8cc2-b3fb7db15e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, Input\n",
    "\n",
    "# Imagine each observation has 4 features (x1, x2, x3, x4)\n",
    "model = Sequential()\n",
    "#model.add(layers.Dense(10, input_dim=4, activation='relu')) # Old syntax\n",
    "model.add(Input(shape=(4,)))  # Define the input shape explicitly\n",
    "model.add(layers.Dense(10, activation='relu'))  # Add a dense layer with 10 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53996867-f14d-44dc-8343-f3c239f2e7b7",
   "metadata": {},
   "source": [
    "### Rule N¬∞2 : The last layer\n",
    "\n",
    "#### Regression\n",
    "\n",
    "You need to create the right layer according to your task. If it's a regression problem. You need to add a linear activation function, so it can take any values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3dde7-ff33-41a6-be13-9ff598630911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only 1 output\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# 13 outputs, you can predict more than one value for one observation. Like a color for each pixel.\n",
    "model.add(layers.Dense(13, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b056ea0-1a8f-47e8-93b3-4f9ea23464f9",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a1e69-e6af-4781-8490-fe4687346936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2 classes (binary)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 13 classes\n",
    "model.add(layers.Dense(13, activation='softmax')) # The sum of each proba will be equal to one ex [0.78, 0.02, etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2bd5b-5308-4816-8094-dbf2abf84ddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Softmax\n",
    "\n",
    "The softmax activation function transforms the raw outputs of the neural network into a vector of probabilities, essentially a probability distribution over the input classes. It's a bit like the sigmoid but for more than two values.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/softmax.jpg\" width=\"45%\" source='https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60' align='center'/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023a53-9ee5-4860-9d37-07e54d68bad3",
   "metadata": {},
   "source": [
    "### Rule N¬∞3 : Experiment\n",
    "\n",
    "In practice, apart from the input size and the last layer, you have to choose:\n",
    "\n",
    "- The number of neurons.\n",
    "- The number of layers.\n",
    "- The activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38df720-1aa2-4281-93ab-1bde41587e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Small exercice: how many parameters in this simple regression task: \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(layers.Dense(10,activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf245d-9128-4815-925d-61261fe64648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de1607-57b8-4f5b-91ca-b69d37361858",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercice:\n",
    "\n",
    "How many parameters in this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9943e-b605-4aea-8ab9-f88da01bc3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(784,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c68ed-7260-4db8-ab3b-f91d32d8c62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e4c24-d998-469a-94c5-a1fadad6b6ed",
   "metadata": {},
   "source": [
    "## Training : Loss and Optimization\n",
    "\n",
    "### Compiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e5138-3877-4c3d-b11f-05c5c73b573a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam') # That's the solver, in DL the default is adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31481a14-26ee-415a-97b2-6d9c1ef202b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0014d91-b801-4594-87e8-894d0cc5f22d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., -3.1, -7.2, 2.1]]) # Same X than before, only one sample\n",
    "y = np.array([[1]])  # only one y\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam') # That's the solver, in DL the default is adam.\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d9c39-6ed2-4d8e-89a9-1bbfc1ecdbb5",
   "metadata": {},
   "source": [
    "- ```batch_size``` is the size of the subset given to the neural network to update the parameters $\\theta$ .Each time the model has reached the batch size, it computes new weights.\n",
    "- One epoch, is one iteration, is when the model has processed all the data you have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39490772-8c4d-403b-bbf8-8f07564235bb",
   "metadata": {},
   "source": [
    "## Example : Face recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055f21c-08bb-41b9-b753-0bcc5e099ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=200, resize=0.25)\n",
    "\n",
    "# 766 images of 31 * 23 pixel black & white\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92379e48-3d5e-44d9-926c-3c3e0df54a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2 different target classes\n",
    "np.unique(faces.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee7e45-852f-4118-8a29-fad8cd260458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,10))\n",
    "for i in range(15):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.title(faces.target_names[faces.target[i]], size=12)\n",
    "    plt.imshow(faces.images[i], cmap=plt.cm.gray)\n",
    "    plt.xticks(()); plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10ab00-6154-4379-837d-53111d5d2503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flatten our 766 images\n",
    "X = faces.images.reshape(766, 31*23) # Each image is now an array of 713 values. And each value is between 0 and 256.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882e0cc-a26f-43f2-86f6-4c8c29271fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = faces.target\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43370963-d638-431c-9239-efef13387eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571b2fe-0927-4d90-9b69-49f59bcca200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd35a0-3bca-4f75-a259-95890f99cdb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, Input\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(713,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97a4c8-5269-49b4-9758-bd2e41324e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy', # The loss used in logistic regression -> Log(Loss)\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec32854-d5ad-4b1d-876b-87262972f758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(scaler.transform(X_test), y_test)\n",
    "# returns [loss, metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a9fec-42e3-40d9-b4cc-5a94c7ac8bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e792e-1586-4bb4-8cfa-0f2eabfb694c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline score (always predict the majority class)\n",
    "530 / (530+236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e42ba-49db-4210-acf1-7681a57fa24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "# model.predict(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74f5df-55a9-4cc1-a33e-47a184d03114",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Deep Learning is nothing more than :\n",
    "    \n",
    "- **Multiple linear regressions** stacked together.\n",
    "- **Non-linear functions**: the activation functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
